services:
  zookeeper:
    image: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    networks:
      - my_network

  kafka:
    image: apache/kafka
    container_name: broker
    command: [start-kafka.sh]
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka:9092,OUTSIDE://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:9093
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      #KAFKA_CREATE_TOPICS: "baeldung:1:1"
    networks:
      - my_network

  # postgres:
  #   image: postgres:15
  #   container_name: postgres
  #   environment:
  #     POSTGRES_USER: postgres
  #     POSTGRES_PASSWORD: password
  #     POSTGRES_DB: airflow
  #   ports:
  #     - "5432:5432"
  #   volumes:
  #     - postgres_data:/var/lib/postgresql/data
  #   networks:
  #     - my_network

  # spark-master:
  #   image: apache/spark
  #   container_name: spark-master
  #   ports:
  #     - "7077:7077"
  #   volumes:
  #      - ./apps:/opt/spark-apps
  #      - ./data:/opt/spark-data
  #   environment:
  #     - SPARK_LOCAL_IP=spark-master
  #     - SPARK_WORKLOAD=master
  #   networks:
  #     - my_network
  # spark-worker-a:
  #   image: apache/spark
  #   container_name: spark-worker-a
  #   ports:
  #     - "7000:7000"
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MASTER=spark://spark-master:7077
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_DRIVER_MEMORY=1G
  #     - SPARK_EXECUTOR_MEMORY=1G
  #     - SPARK_WORKLOAD=worker
  #     - SPARK_LOCAL_IP=spark-worker-a
  #   volumes:
  #      - ./apps:/opt/spark-apps
  #      - ./data:/opt/spark-data
  #   networks:
  #      - my_network
  # spark-worker-b:
  #   image: apache/spark
  #   container_name: spark-worker-b
  #   ports:
  #     - "7001:7000"
  #   depends_on:
  #     - spark-master
  #   environment:
  #     - SPARK_MASTER=spark://spark-master:7077
  #     - SPARK_WORKER_CORES=1
  #     - SPARK_WORKER_MEMORY=1G
  #     - SPARK_DRIVER_MEMORY=1G
  #     - SPARK_EXECUTOR_MEMORY=1G
  #     - SPARK_WORKLOAD=worker
  #     - SPARK_LOCAL_IP=spark-worker-b
  #   volumes:
  #       - ./apps:/opt/spark-apps
  #       - ./data:/opt/spark-data 
  #   networks:
  #       - my_network

  # airflow-webserver:
  #   image: apache/airflow:2.8.3
  #   container_name: airflow-webserver
  #   restart: always
  #   depends_on:
  #     - postgres
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Use LocalExecutor instead of CeleryExecutor
  #     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:password@postgres/airflow
  #     - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY  # Generate a Fernet key and replace it here
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  #   ports:
  #     - "8080:8080"
  
  # airflow-scheduler:
  #   image: apache/airflow:2.8.3
  #   container_name: airflow-scheduler
  #   restart: always
  #   depends_on:
  #     - postgres
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Use LocalExecutor
  #     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:password@postgres/airflow
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  
  # airflow-init:
  #   image: apache/airflow:2.8.3
  #   container_name: airflow-init
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Use LocalExecutor
  #     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:password@postgres/airflow
  #     - AIRFLOW__CORE__FERNET_KEY=8zyd969nRuMPKtE5TKKAVhs8HkfMqnn0SRYPMmdVjQc=
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  #   entrypoint: >
  #     bash -c "
  #     airflow db upgrade &&
  #     airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
  #     airflow scheduler
  #     "
  #   depends_on:
  #     - postgres

#volumes:
#  postgres_data:  # Ensure this is correctly defined

networks:
  my_network: