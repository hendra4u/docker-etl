version: '3.8'

services:
  zookeeper:
    image: zookeeper:3.6.1
    container_name: zookeeper
    expose:
      - "2181"
    volumes:
      - kafka_zookeeper:/opt/zookeeper-3.6.1/data
    networks:
      kafkanet:
        ipv4_address: 172.25.0.11

  kafka1:
    image: wurstmeister/kafka:2.12-2.2.0
    container_name: kafka1
    command: [start-kafka.sh]
    expose:
      - "8080"
      - "9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: 172.25.0.12
      KAFKA_ZOOKEEPER_CONNECT: 172.25.0.11:2181
      KAFKA_ADVERTISED_PORT: 9092
    volumes:
      - kafka_kafka1:/opt/kafka_2.12-2.2.0/logs
    networks:
      kafkanet:
        ipv4_address: 172.25.0.12
    depends_on:
      - "zookeeper"

  kafka2:
    image: wurstmeister/kafka:2.12-2.2.0
    container_name: kafka2
    command: [start-kafka.sh]
    expose:
      - "8080"
      - "9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: 172.25.0.13
      KAFKA_ZOOKEEPER_CONNECT: 172.25.0.11:2181
      KAFKA_ADVERTISED_PORT: 9092
    volumes:
      - kafka_kafka2:/opt/kafka_2.12-2.2.0/logs
    depends_on:
      - "zookeeper"
    networks:
      kafkanet:
        ipv4_address: 172.25.0.13

  spark:
    image: gettyimages/spark:2.4.1-hadoop-3.0
    container_name: spark
    volumes:
      - ./:/app
    networks:
      kafkanet:
        ipv4_address: 172.25.0.14

  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      #- my_network
      - kafkanet
  
  # airflow-webserver:
  #   image: apache/airflow:2.8.3
  #   container_name: airflow-webserver
  #   restart: always
  #   depends_on:
  #     - postgres
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Use LocalExecutor instead of CeleryExecutor
  #     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:password@postgres/airflow
  #     - AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY  # Generate a Fernet key and replace it here
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  #   ports:
  #     - "8080:8080"
  
  # airflow-scheduler:
  #   image: apache/airflow:2.8.3
  #   container_name: airflow-scheduler
  #   restart: always
  #   depends_on:
  #     - postgres
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Use LocalExecutor
  #     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:password@postgres/airflow
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  
  # airflow-init:
  #   image: apache/airflow:2.8.3
  #   container_name: airflow-init
  #   environment:
  #     - AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Use LocalExecutor
  #     - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:password@postgres/airflow
  #     - AIRFLOW__CORE__FERNET_KEY=8zyd969nRuMPKtE5TKKAVhs8HkfMqnn0SRYPMmdVjQc=
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./logs:/opt/airflow/logs
  #     - ./plugins:/opt/airflow/plugins
  #   entrypoint: >
  #     bash -c "
  #     airflow db upgrade &&
  #     airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
  #     airflow scheduler
  #     "
  #   depends_on:
  #     - postgres

volumes:
  kafka_zookeeper:
  kafka_kafka1:
  kafka_kafka2:
  postgres_data:  # Ensure this is correctly defined

networks:
  kafkanet:
    name: kafkanet
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/16

  #my_network:exit